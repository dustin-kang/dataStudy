# 앙상블(Ensemble)
> - 앙상블은 여러개의 **기본 모델(Weak Learner)들을 결합**하여 **하나의 좋은 성능을 지닌 모델(Strong Learner)** 로 만들어 내는 것을 의미합니다. (정확도가 낮은 약한 모델을 조합하여 정확도를 높이는 방법)
> - 대표적으로 `Bagging`, `Boosting`, `Stacking`이 있습니다. 

<img src="https://velog.velcdn.com/images/qqo222/post/1dcd1b5a-72b2-4c2f-abff-21895385beff/image.png" width=500>

## 배깅(Bagging)
> 부트스트랩을 통해 샘플을 **여러개 뽑아**(부트스트랩, 복원랜덤 샘플링) 각 모델을 학습 시켜 결과물을 **집계**(Aggregation)하는 방식 입니다.

- **부트스트랩** : 원본 데이터서 복원 추출 후 기록하고 제자리에 두면서 반복하여 하나의 세트를 만드는 과정
  - **Out of Bag Sample** : 복원추출에서 뽑이지 않은 샘플들을 테스트 세트로 지정합니다. `model.obb_score_`(split이 아님)
  - **시계열 데이터** : 시간 및 순서가 있는 데이터는 연관성이 있으므로 무작위로 선택할 수 없습니다.
- **집계** : 회귀문제는 **평균**으로 결과를 집계하고 분류문제는 가장 많은 클래스로 **투표 방식**을 통해 집계합니다.
-  **랜덤 포레스트**로 활용할 수 있습니다.

## 부스팅(Boosting)
> 이전 모델의 **오답에 가중치를 높게 부여**하여 다음 모델을 학습하는 방법입니다. (오답에 더 집중하여 학습시키기 때문에 배깅에 비해 정확도가 높습니다.)

- 장점 : 배깅에 비해 정확도가 높은 편입니다.
- 단점 : 틀렸던 부분을 반복적으로 학습하기 때문에 오버피팅에 문제가 있으며 Outlier에 취약하고 속도가 느립니다.
- GBM(Gradient Boosting), XGBoost, AdaBoostm GradientBoost 등의 알고리즘이 있습니다.


## 스태킹(Stacking)
> 각기 다른 모델들이 예측한 결과값을 **다시 학습데이터로 사용해** 모델을 만드는 방법입니다.

- 장점 : 개별 모델의 결과를 결합하여 성능을 높일 수 있습니다.
- 단점 :  같은 데이터셋을 통한 결과를 또 학습하기 때문에 오버피팅이 문제가 있습니다. (교차검증으로 문제를 해결할 수 있습니다.)


## 배깅과 부스팅 차이

||배깅|부스팅|
|:---:|:---:|:---:|
|방식|병렬적|순차적|
|장점|성능 향상(평균)|에러 감소|
|단점|에러 존재|느린속도와 과적합|
|사용|Overfitting 해결|성능 해결|
|모델|랜덤포레스트|XGBoost|


# Reference
- [🔗 앙상블 학습 - 귀퉁이 서재](https://bkshin.tistory.com/entry/머신러닝-11-앙상블-학습-Ensemble-Learning-배깅Bagging과-부스팅Boosting)
- [🔗 앙상블 기법과 배깅, 부스팅, 스태킹 - 데분데싸](https://data-analysis-science.tistory.com/61)
- [🔗 부트스트랩(Bootstrap) - 귀퉁이 서재](https://bkshin.tistory.com/entry/DATA-12?category=1042793)
