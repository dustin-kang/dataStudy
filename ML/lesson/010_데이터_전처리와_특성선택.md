# ë°ì´í„° íŠ¹ì„± ì„ íƒ
## Feature Selection
ë¨¸ì‹ ëŸ¬ë‹ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ë°˜ë“œì‹œ í•„ìš”í•œ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ë¡œ, ëª¨ë“  Featuresë¥¼ ì„ íƒí•˜ê¸°ì—ëŠ” ë©”ëª¨ë¦¬ ì¸¡ë©´ì—ì„œë‚˜ Computing Power ì¸¡ë©´ì—ì„œë‚˜ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë¼ ì¼ë¶€ í•„ìš”í•œ í”¼ì²˜ë“¤ë§Œ ì„ íƒí•˜ëŠ” ì•„ì´ë””ì–´ì…ë‹ˆë‹¤.

> [ì°¨ì›ì¶•ì†Œ í˜ì´ì§€](https://github.com/dustin-kang/dataStudy/blob/main/ML/lesson/004_PCAì™€_ì°¨ì›ì¶•ì†Œ.md#ì°¨ì›-ì¶•ì†Œ-ê¸°ë²•ì˜-ì¢…ë¥˜)ì— ë‚˜ì™€ìˆë“¯ì´ íŠ¹ì„± ê³µí•™(Feature Enginnering)ì„ í†µí•´ [ì°¨ì›ì˜ ì €ì£¼](https://bioinformaticsandme.tistory.com/197)ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

|í‘œí˜„|ì •ì˜|
|:---:|:---|
|Feature Enginnering|ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ì—ì„œ Featureë¥¼ ë³€í˜•/ìƒì„±|
|Feature Extraction|ì°¨ì›ì¶•ì†Œ ë“± ì„¸ë¡œìš´ ì¤‘ìš” Featureë¥¼ ì¶”ì¶œ (ìœ ìš©í•˜ê²Œ ë§Œë“ ë‹¤.)|
|Feature Selection|ê¸°ì¡´ Featureì—ì„œ ì›í•˜ëŠ” Featureë§Œ ì„ íƒí•˜ê³  ë³€ê²½í•˜ì§€ ì•ŠìŒ (ìœ ìš©í•œ ê²ƒì„ ê³ ë¥¸ë‹¤.)|


### íŠ¹ì„± ì„ íƒì˜ ì¥ì 
- ì‚¬ìš©ìê°€ ë” í•´ì„í•˜ê¸° ì‰½ê²Œ ëª¨ë¸ì„ **ë‹¨ìˆœí™”**í•©ë‹ˆë‹¤.
- í›ˆë ¨ ì‹œê°„ì´ ì¶•ì†Œë©ë‹ˆë‹¤.
- ì°¨ì›ì˜ ì €ì£¼ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
- Overfittingì„ ì¤„ì—¬ ì¼ë°˜í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë°©ë²•ë¡ 
### Wrapped Method
> ì˜ˆì¸¡ ì •í™•ë„ ì¸¡ë©´ì—ì„œ **ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ Feature Subset(íŠ¹ì„± ì§‘í•©)ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤. 

- í…ŒìŠ¤íŠ¸ ì§„í–‰ì„ ìœ„í•´ ê¸°ì¡´ ë°ì´í„°ì—ì„œ Hold-out Setë¥¼ ë”°ë¡œ ë‘ì–´ì•¼í•©ë‹ˆë‹¤.
- ë°˜ë³µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ í›ˆë ¨ìœ¼ë¡œ **ì‹œê°„ ë° ë¹„ìš©ì´ ë†’ê²Œ ë°œìƒ**í•˜ë‚˜ ê²°êµ­ì—” **ìµœì ì˜ íŠ¹ì„± ì§‘í•©ë“¤ì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸**ì— ë°”ëŒì§í•œ ë°©ë²•ì…ë‹ˆë‹¤. (í•˜ì§€ë§Œ ê·¸ë§Œí¼, ëª¨ë¸ë„ ì¢‹ì•„ì•¼ê² ì£ ?)

<img width="820" alt="image" src="https://user-images.githubusercontent.com/55238671/212636125-86a566e6-14d8-474b-8370-1522f4480c24.png">


> ğŸ” ë‹¤ì–‘í•œ Wrapped Method ë°©ë²•ë¡ 
> - Exhaustive search(ì™„ì „ íƒìƒ‰) : íŠ¹ì„±ì˜ ëª¨ë“  ê°€ëŠ¥í•œ ê²°í•©ì„ ê³„ì‚°í•©ë‹ˆë‹¤. (ì‹¤ì§ˆì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥)
> - Sequential Method(ìˆœì°¨ íƒìƒ‰) : ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŠ¹ì„±ì„ ìœ„í•´ í•˜ë‚˜ì”© ì œê±°í•´ ë‚˜ê°‘ë‹ˆë‹¤.(RFE recursive feature elimination)
> - Forward Selection(ì „ì§„ ì„ íƒ) : í•˜ë‚˜ë„ ì„ íƒ ì•ˆëœ ìƒíƒœì—ì„œ í•˜ë‚˜ì”© ì¶”ê°€í•´ê°€ëŠ” ë°©ë²• (SFS: sequential feature selection)
> - Backward Selection(í›„ì§„ ì„ íƒ) : ëª¨ë“  íŠ¹ì„±ì„ ê°€ì§€ê³  ì‹œì‘í•´ í•˜ë‚˜ì”© ì œê±°í•´ ë‚˜ê°€ëŠ” ë°©ë²•
> - Stochastic Method(í™•ë¥ ë¡ ì  ë°©ë²•) : Local Optimaë¥¼ í”¼í•˜ê¸° ìœ„í•´ íƒìƒ‰ ì ˆì°¨ì—ì„œ ì„ì˜ì ìœ¼ë¡œ íƒí•˜ëŠ” ë°©ë²•




### Filter Method
> Features ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì•Œì•„ë‚¸ í›„, **ë†’ì€ ìƒê´€ê³„ìˆ˜ë¥¼ ê°€ì§€ëŠ” Featuresë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤.

- Information Gain, Chi-Square test, Fisher score, Correlation Coefficient, Variance Threshold

<img width="820" alt="image" src="https://user-images.githubusercontent.com/55238671/212636175-2a9ad943-a6db-4c75-ae0f-edc311bda71e.png">


#### ğŸ’» `SelectKBest`

```py
from sklearn.feature_selection import f_regression, SelectKBest
selector = SelectKBest(score_func=f_regression, k=4)

X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)

# Feature Selection
all_names = X_train.columns # Train Features 

selected_filter = selector.get_support() # Selector Filter

selected_names = all_names[selected_filter] # Selected Features
# unselected_names = all_names[~selected_filter]

print(selected_names)
```

### Embedded Method
> ìœ„ ë‘ê°€ì§€ ë°©ë²•ì˜ ì¥ì ì„ í•©ì¹œ ë°©ë²•ìœ¼ë¡œ ê°ê°ì˜ íŠ¹ì„±ë“¤ì„ í•™ìŠµí•˜ì—¬, **ëª¨ë¸ì˜ ì •í™•ë„ì— ê¸°ì—¬í•˜ëŠ” íŠ¹ì„±ì„ ì„ íƒ**í•©ë‹ˆë‹¤.

- ê³„ìˆ˜ê°€ 0ì´ ì•„ë‹Œ íŠ¹ì„±ì´ ì„ íƒë˜ì–´ ë” ë‚®ì€ ë³µì¡ì„±ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•´ í•™ìŠµ ì ˆì°¨ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
- Lasso, Ridge, Elastic Net, `SelectFromModel`(Decision Tree ê¸°ë°˜ ëª¨ë¸)

<img width="820" alt="image" src="https://user-images.githubusercontent.com/55238671/212636214-efba1610-7995-4909-8f4e-5724b9c6f3ed.png">


#### ğŸ’» `SelectFromModel`
```py
from sklearn.feature_selection import SelectFormModel

select = SelectFromModel(Model, threshold="Mean")
select.fit(X_train, y_train)
X_train_selected = select.transform(X_train)
X_test_selected = select.transform(X_test)
select.get_support()
```


## ì£¼ì˜ì 

- í›ˆë ¨í•  ë°ì´í„°ì—ì„œ featureë¥¼ ê³ ë¥¸ë‹¤ë©´, í›ˆë ¨ ë°ì´í„°ì— ê³¼ì í•©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ê·¸ëŸ¬ë¯€ë¡œ í›ˆë ¨ ë°ì´í„°, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì œì™¸í•œ ë°ì´í„°ì—ì„œ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
- ëª¨ë“  ë°ì´í„°ì—ì„œ feature selectionì„ ì§„í–‰í•˜ë©´, êµì°¨ ê²€ì¦ì—ì„œ ë˜‘ê°™ì´ ì„ íƒëœ featureë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë¯€ë¡œ ê²°ê³¼ê°€ í¸í–¥ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# Reference
- [ğŸ”— Feature Selectionì´ë€? - Subinium](https://subinium.github.io/feature-selection/)
- [ğŸ”— ì°¨ì› ì¶•ì†Œ - Feature Selection 3ê°€ì§€ ë°©ë²• - ë°€ë˜ì˜ ì½”ë”©ë¶](https://firework-ham.tistory.com/48)
- [ğŸ”— Sklearn Feature Selection Document](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)
- [ğŸ’» Feature Selection ì‹¤ìŠµ](https://github.com/dustin-kang/dataStudy/blob/main/ML/practice/006_feature_selection.ipynb)
