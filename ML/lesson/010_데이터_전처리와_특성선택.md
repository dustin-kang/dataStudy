# 데이터 특성 선택
## Feature Selection
머신러닝의 성능을 높이기 위해 반드시 필요한 기술 중 하나로, 모든 Features를 선택하기에는 메모리 측면에서나 Computing Power 측면에서나 매우 비효율적이라 일부 필요한 피처들만 선택하는 아이디어입니다.

> [차원축소 페이지](https://github.com/dustin-kang/dataStudy/blob/main/ML/lesson/004_PCA와_차원축소.md#차원-축소-기법의-종류)에 나와있듯이 특성 공학(Feature Enginnering)을 통해 [차원의 저주](https://bioinformaticsandme.tistory.com/197)를 방지할 수 있습니다.

|표현|정의|
|:---:|:---|
|Feature Enginnering|도메인 지식을 활용하여 데이터에서 Feature를 변형/생성|
|Feature Extraction|차원축소 등 세로운 중요 Feature를 추출 (유용하게 만든다.)|
|Feature Selection|기존 Feature에서 원하는 Feature만 선택하고 변경하지 않음 (유용한 것을 고른다.)|


### 특성 선택의 장점
- 사용자가 더 해석하기 쉽게 모델을 **단순화**합니다.
- 훈련 시간이 축소됩니다.
- 차원의 저주를 방지합니다.
- Overfitting을 줄여 일반화 할 수 있습니다.

## 방법론
### Wrapped Method
> 예측 정확도 측면에서 **가장 성능이 좋은 Feature Subset(특성 집합)을 가져오는 방법**입니다. 

- 테스트 진행을 위해 기존 데이터에서 Hold-out Set를 따로 두어야합니다.
- 반복적인 머신러닝 훈련으로 **시간 및 비용이 높게 발생**하나 결국엔 **최적의 특성 집합들을 찾아낼 수 있기 때문**에 바람직한 방법입니다. (하지만 그만큼, 모델도 좋아야겠죠?)

<img width="820" alt="image" src="https://user-images.githubusercontent.com/55238671/212636125-86a566e6-14d8-474b-8370-1522f4480c24.png">


> 🔍 다양한 Wrapped Method 방법론
> - Exhaustive search(완전 탐색) : 특성의 모든 가능한 결합을 계산합니다. (실질적으로 불가능)
> - Sequential Method(순차 탐색) : 예측 정확도를 최대화하는 특성을 위해 하나씩 제거해 나갑니다.(RFE recursive feature elimination)
> - Forward Selection(전진 선택) : 하나도 선택 안된 상태에서 하나씩 추가해가는 방법 (SFS: sequential feature selection)
> - Backward Selection(후진 선택) : 모든 특성을 가지고 시작해 하나씩 제거해 나가는 방법
> - Stochastic Method(확률론적 방법) : Local Optima를 피하기 위해 탐색 절차에서 임의적으로 택하는 방법




### Filter Method
> Features 간의 상관관계를 알아낸 후, **높은 상관계수를 가지는 Features를 사용하는 방법**입니다.

- Information Gain, Chi-Square test, Fisher score, Correlation Coefficient, Variance Threshold

<img width="820" alt="image" src="https://user-images.githubusercontent.com/55238671/212636175-2a9ad943-a6db-4c75-ae0f-edc311bda71e.png">


#### 💻 `SelectKBest`

```py
from sklearn.feature_selection import f_regression, SelectKBest
selector = SelectKBest(score_func=f_regression, k=4)

X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)

# Feature Selection
all_names = X_train.columns # Train Features 

selected_filter = selector.get_support() # Selector Filter

selected_names = all_names[selected_filter] # Selected Features
# unselected_names = all_names[~selected_filter]

print(selected_names)
```

### Embedded Method
> 위 두가지 방법의 장점을 합친 방법으로 각각의 특성들을 학습하여, **모델의 정확도에 기여하는 특성을 선택**합니다.

- 계수가 0이 아닌 특성이 선택되어 더 낮은 복잡성으로 모델을 훈련해 학습 절차를 최적화합니다.
- Lasso, Ridge, Elastic Net, `SelectFromModel`(Decision Tree 기반 모델)

<img width="820" alt="image" src="https://user-images.githubusercontent.com/55238671/212636214-efba1610-7995-4909-8f4e-5724b9c6f3ed.png">


#### 💻 `SelectFromModel`
```py
from sklearn.feature_selection import SelectFormModel

select = SelectFromModel(Model, threshold="Mean")
select.fit(X_train, y_train)
X_train_selected = select.transform(X_train)
X_test_selected = select.transform(X_test)
select.get_support()
```


## 주의점

- 훈련할 데이터에서 feature를 고른다면, 훈련 데이터에 과적합될 수 있습니다 그러므로 훈련 데이터, 테스트 데이터를 제외한 데이터에서 선택하는 것이 중요합니다.
- 모든 데이터에서 feature selection을 진행하면, 교차 검증에서 똑같이 선택된 feature를 사용하게 되므로 결과가 편향될 수 있습니다.


# Reference
- [🔗 Feature Selection이란? - Subinium](https://subinium.github.io/feature-selection/)
- [🔗 차원 축소 - Feature Selection 3가지 방법 - 밀래의 코딩북](https://firework-ham.tistory.com/48)
- [🔗 Sklearn Feature Selection Document](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)
- [💻 Feature Selection 실습](https://github.com/dustin-kang/dataStudy/blob/main/ML/practice/006_feature_selection.ipynb)
