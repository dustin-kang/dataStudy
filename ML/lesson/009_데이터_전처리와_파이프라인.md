# Data PreProcessing

- [ ] 데이터 클린징 (오류 데이터 수정)
- [ ] 결손값 처리 (`Null/NaN` 데이터 처리 `imputer`)
- [ ] 데이터 인코딩 (`LabelEncoder`, `One-Hot Encoder`)
- [ ] 데이터 스케일링 (`Min-MaxSclaer`, `StandardSclaer` `RobustScaler`)
- [ ] 이상치 제거 (`IQR`)
- [ ] Feature 선택 및 추출 가공 (`Feaute Selection`)

## 결측치 처리
> 머신러닝에서 **결측치(NaN/Null)를 처리** 하기 위해 다음 두가지 방식을 많이 사용합니다. 
> - Single Imputation (Imputer)
> - Multiveriate Imputation (MICE)

### SimpleImputer

```py
from sklearn.imputer import SimpleImputer

imputer = SimpleImputer(strategy= 'mean')
imputer.fit_transform(train_df)
```
- `missing_values` : 해당 데이터 내에 결측치 값
- `strategy` : 대표값을 정할 수 있습니다. _ex) `mean` `median` `most_frequent` `constant`(정해진 값)_
- `fill_value` : `strategy`로 `constant` 설정시, 정해주면 됩니다. 
- `axis` : 방향 설정 (열 = 0)
- `copy` : `True`일 경우 X의 복사본이 만들어진다.

[🔗 `Simpleimputer` 사이킷런 공식 도큐먼트](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)

### IterativeImputer
```py
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(max_iter = 10, random_state = 0)
imputer.fit_transform(tran_df)
```
- `max_iter` : 최대 반복 수를 의미합니다.
- 라운드 로빈 방식(동등한 기회)으로 각 Features에 대해 **회귀 분석을 진행하여 결측값을 예측**하는 시스템이다.

[🔗 `Iterativeimputer` 사이킷런 공식 도큐먼트](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)

> 🔍 `iterativeImputer` 를 사용하면 R data Science 생태계에서 `missForest`등 다양한 기법으로 사용할 수 있습니다.

> 🔍 `fancy impute` 기법의 MICE는 더이상 지원하지 않습니다. → 이제는 `fancyimpute.iterativeImputer` 로 실행합니다. [🔗GeeksforGeeks](https://stackoverflow.com/questions/54059964/can-not-use-mice-from-fancyimputer-python)

[🔗 Iterative Imputer를 사용하여 MICE 알고리즘 구현하는 방법 ](https://www.numpyninja.com/post/how-to-implement-mice-algorithm-using-iterative-imputer-to-handle-missing-values)
[🔗 결측치(Missing values, Nulls) 처리에 대해서 (Imputation): SimpleImputer, IterativeImputer, MICE .. - jee-9](https://velog.io/@jee-9/결측치Nulls-처리에-대해서-Imputation-Single-MICE)

## 이상치 제거
> 이상치가 존재하는 데이터로 학습한 모델은 왜곡된 결과를 도출해서 성능이 떨어질 수 있습니다.
### IQR로 이상치 탐지하기
> 이상치를 탐지하는 방법은 **백분위수(Percentile)를 사용**하거나 클러스터링(Clustering) 기반으로 판단하는 방법들이 존재합니다.

```py
# 이상치를 탐색하는 코드
from collections import Counter # 리스트 객체를 받아 중복갯수를 딕셔너리로 반환

def detect_outliers(df, n, features):
    outlier_indices = []
    for col in features:
        Q1 = np.percentile(df[col], 25)
        Q3 = np.percentile(df[col], 75)
        IQR = Q3 - Q1
        
        outlier_step = 1.5 * IQR
        
        # 이상치 = 최소제한선보다 작고, 최대제한선 보다 큰 값
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        outlier_indices.extend(outlier_list_col)

    outlier_indices = Counter(outlier_indices)
    # n 이상인 갯수
    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)
        
    return multiple_outliers
        
Outliers_to_drop = detect_outliers(df_train, 2, ["변수명"])

# 이상치 출력하기
df_train.loc[Outliers_to_drop]

# 이상치 제거하기
df_train = df_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)
```

## 파이프라인 구축

> 코드의 간결화와 가독성, 안정성, 자동화를 위해 파이프라인을 구축합니다.

```py
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression())
])

model = pipeline.names_steps['classifier']
model.fit(X_train, y_train)
print('검증세트 정확도', model.score(X_val, y_val))

y_pred = model.predict(X_test)
accuracy_score(y_test,y_preds)

# 교차검증과 함께 사용하여 파라미터를 재구성할 수 있습니다.
pipeline.set_params(clf__penalty='l2',vect__max_df=0.9,clf__dual=True)
pipeline.fit(X_train,y_train)
y_preds = pipeline.predict(X_test)

f1_score(y_test, y_preds, average='micro')
```
- `named_steps` : 이름을 지정할 수 있습니다.

### 중간 결과 확인 방법
```py
## 중간 결과 보기 - named_steps 이용한 방법
## 선택된 변수 보기
print('선택된 변수 :', pipeline.named_steps['Feature_Selection'].get_feature_names_out())
print()
## 표준화가 잘되었는지 확인하기
var_selected = pipeline.named_steps['Feature_Selection'].get_feature_names_out()
X_selected = X[:,[int(x.replace('x','')) for x in var_selected]]
X_transformed = pipeline.named_steps['Standardization'].transform(X_selected)
print(X_transformed[:5, :])
```
[🔗 평생 데이터 분석 하고픈 꽁냥이](https://zephyrus1111.tistory.com/254)
[🔗 `pipeline` 사이킷런 공식 도큐먼트 ](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)

