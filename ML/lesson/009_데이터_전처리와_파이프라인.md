# Data PreProcessing

- [ ] ë°ì´í„° í´ë¦°ì§• (ì˜¤ë¥˜ ë°ì´í„° ìˆ˜ì •)
- [ ] ê²°ì†ê°’ ì²˜ë¦¬ (`Null/NaN` ë°ì´í„° ì²˜ë¦¬ `imputer`)
- [ ] ë°ì´í„° ì¸ì½”ë”© (`LabelEncoder`, `One-Hot Encoder`)
- [ ] ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (`Min-MaxSclaer`, `StandardSclaer` `RobustScaler`)
- [ ] ì´ìƒì¹˜ ì œê±° (`IQR`)
- [ ] Feature ì„ íƒ ë° ì¶”ì¶œ ê°€ê³µ (`Feaute Selection`)

## ê²°ì¸¡ì¹˜ ì²˜ë¦¬
> ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ **ê²°ì¸¡ì¹˜(NaN/Null)ë¥¼ ì²˜ë¦¬** í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ë‘ê°€ì§€ ë°©ì‹ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤. 
> - Single Imputation (Imputer)
> - Multiveriate Imputation (MICE)

### SimpleImputer

```py
from sklearn.imputer import SimpleImputer

imputer = SimpleImputer(strategy= 'mean')
imputer.fit_transform(train_df)
```
- `missing_values` : í•´ë‹¹ ë°ì´í„° ë‚´ì— ê²°ì¸¡ì¹˜ ê°’
- `strategy` : ëŒ€í‘œê°’ì„ ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. _ex) `mean` `median` `most_frequent` `constant`(ì •í•´ì§„ ê°’)_
- `fill_value` : `strategy`ë¡œ `constant` ì„¤ì •ì‹œ, ì •í•´ì£¼ë©´ ë©ë‹ˆë‹¤. 
- `axis` : ë°©í–¥ ì„¤ì • (ì—´ = 0)
- `copy` : `True`ì¼ ê²½ìš° Xì˜ ë³µì‚¬ë³¸ì´ ë§Œë“¤ì–´ì§„ë‹¤.

[ğŸ”— `Simpleimputer` ì‚¬ì´í‚·ëŸ° ê³µì‹ ë„íë¨¼íŠ¸](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)

### IterativeImputer
```py
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(max_iter = 10, random_state = 0)
imputer.fit_transform(tran_df)
```
- `max_iter` : ìµœëŒ€ ë°˜ë³µ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹(ë™ë“±í•œ ê¸°íšŒ)ìœ¼ë¡œ ê° Featuresì— ëŒ€í•´ **íšŒê·€ ë¶„ì„ì„ ì§„í–‰í•˜ì—¬ ê²°ì¸¡ê°’ì„ ì˜ˆì¸¡**í•˜ëŠ” ì‹œìŠ¤í…œì´ë‹¤.

[ğŸ”— `Iterativeimputer` ì‚¬ì´í‚·ëŸ° ê³µì‹ ë„íë¨¼íŠ¸](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)

> ğŸ” `iterativeImputer` ë¥¼ ì‚¬ìš©í•˜ë©´ R data Science ìƒíƒœê³„ì—ì„œ `missForest`ë“± ë‹¤ì–‘í•œ ê¸°ë²•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> ğŸ” `fancy impute` ê¸°ë²•ì˜ MICEëŠ” ë”ì´ìƒ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. â†’ ì´ì œëŠ” `fancyimpute.iterativeImputer` ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. [ğŸ”—GeeksforGeeks](https://stackoverflow.com/questions/54059964/can-not-use-mice-from-fancyimputer-python)

[ğŸ”— Iterative Imputerë¥¼ ì‚¬ìš©í•˜ì—¬ MICE ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ëŠ” ë°©ë²• ](https://www.numpyninja.com/post/how-to-implement-mice-algorithm-using-iterative-imputer-to-handle-missing-values)
[ğŸ”— ê²°ì¸¡ì¹˜(Missing values, Nulls) ì²˜ë¦¬ì— ëŒ€í•´ì„œ (Imputation): SimpleImputer, IterativeImputer, MICE .. - jee-9](https://velog.io/@jee-9/ê²°ì¸¡ì¹˜Nulls-ì²˜ë¦¬ì—-ëŒ€í•´ì„œ-Imputation-Single-MICE)

## ì´ìƒì¹˜ ì œê±°
> ì´ìƒì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸ì€ ì™œê³¡ëœ ê²°ê³¼ë¥¼ ë„ì¶œí•´ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
### IQRë¡œ ì´ìƒì¹˜ íƒì§€í•˜ê¸°
> ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ë°©ë²•ì€ **ë°±ë¶„ìœ„ìˆ˜(Percentile)ë¥¼ ì‚¬ìš©**í•˜ê±°ë‚˜ í´ëŸ¬ìŠ¤í„°ë§(Clustering) ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ë°©ë²•ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

```py
# ì´ìƒì¹˜ë¥¼ íƒìƒ‰í•˜ëŠ” ì½”ë“œ
from collections import Counter # ë¦¬ìŠ¤íŠ¸ ê°ì²´ë¥¼ ë°›ì•„ ì¤‘ë³µê°¯ìˆ˜ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜

def detect_outliers(df, n, features):
    outlier_indices = []
    for col in features:
        Q1 = np.percentile(df[col], 25)
        Q3 = np.percentile(df[col], 75)
        IQR = Q3 - Q1
        
        outlier_step = 1.5 * IQR
        
        # ì´ìƒì¹˜ = ìµœì†Œì œí•œì„ ë³´ë‹¤ ì‘ê³ , ìµœëŒ€ì œí•œì„  ë³´ë‹¤ í° ê°’
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        outlier_indices.extend(outlier_list_col)

    outlier_indices = Counter(outlier_indices)
    # n ì´ìƒì¸ ê°¯ìˆ˜
    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)
        
    return multiple_outliers
        
Outliers_to_drop = detect_outliers(df_train, 2, ["ë³€ìˆ˜ëª…"])

# ì´ìƒì¹˜ ì¶œë ¥í•˜ê¸°
df_train.loc[Outliers_to_drop]

# ì´ìƒì¹˜ ì œê±°í•˜ê¸°
df_train = df_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)
```

## íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

> ì½”ë“œì˜ ê°„ê²°í™”ì™€ ê°€ë…ì„±, ì•ˆì •ì„±, ìë™í™”ë¥¼ ìœ„í•´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

```py
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression())
])

model = pipeline.names_steps['classifier']
model.fit(X_train, y_train)
print('ê²€ì¦ì„¸íŠ¸ ì •í™•ë„', model.score(X_val, y_val))

y_pred = model.predict(X_test)
accuracy_score(y_test,y_preds)

# êµì°¨ê²€ì¦ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
pipeline.set_params(clf__penalty='l2',vect__max_df=0.9,clf__dual=True)
pipeline.fit(X_train,y_train)
y_preds = pipeline.predict(X_test)

f1_score(y_test, y_preds, average='micro')
```
- `named_steps` : ì´ë¦„ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì¤‘ê°„ ê²°ê³¼ í™•ì¸ ë°©ë²•
```py
## ì¤‘ê°„ ê²°ê³¼ ë³´ê¸° - named_steps ì´ìš©í•œ ë°©ë²•
## ì„ íƒëœ ë³€ìˆ˜ ë³´ê¸°
print('ì„ íƒëœ ë³€ìˆ˜ :', pipeline.named_steps['Feature_Selection'].get_feature_names_out())
print()
## í‘œì¤€í™”ê°€ ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸°
var_selected = pipeline.named_steps['Feature_Selection'].get_feature_names_out()
X_selected = X[:,[int(x.replace('x','')) for x in var_selected]]
X_transformed = pipeline.named_steps['Standardization'].transform(X_selected)
print(X_transformed[:5, :])
```
[ğŸ”— í‰ìƒ ë°ì´í„° ë¶„ì„ í•˜ê³ í”ˆ ê½ëƒ¥ì´](https://zephyrus1111.tistory.com/254)
[ğŸ”— `pipeline` ì‚¬ì´í‚·ëŸ° ê³µì‹ ë„íë¨¼íŠ¸ ](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)

