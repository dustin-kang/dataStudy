#  Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)

> ê±°ë¦¬ë¥¼ ì œê³±í•˜ì—¬ êµ¬í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ **ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ë¥¼ ì˜ˆì¸¡**í•˜ê¸° ìœ„í•´ 0ê³¼ 1ì‚¬ì´ì˜ ë²”ìœ„ë¡œ ë‚˜íƒœë‚´ì–´ í‘œí˜„í•˜ëŠ” ë¶„ë¥˜ ëª¨ë¸ì…ë‹ˆë‹¤. _ë¬´í•œëŒ€ì—ì„œ ê±°ë¦¬ë¥¼ êµ¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ìµœì†Œ ì œê³±ë²•ì„ ì‚¬ìš©í•˜ì§€ ëª»í•©ë‹ˆë‹¤._


## ë¡œì§“ ë³€í™˜ (Logit Transformation)

<img width="430" alt="image" src="https://user-images.githubusercontent.com/55238671/214308945-1b44783c-2bb6-4c37-ac60-7bf3f0365b5f.png">


- íšŒê·€ ê³„ìˆ˜ì˜ ì˜ë¯¸ë¥¼ í•´ì„í•˜ê¸° ìœ„í•´ `ë¡œì§“ë³€í™˜`ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì„ í˜•í•¨ìˆ˜ë¡œ ë°”ê¿‰ë‹ˆë‹¤. 
- ì˜¤ì¦ˆì— ë¡œê·¸ë¥¼ ì·¨í•œ ë°©ì‹ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

### ì˜¤ì¦ˆ(Odds)
ì˜¤ì¦ˆëŠ” ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì„ ì‚¬ê±´ì´ ë°œìƒí•˜ì§€ ì•Šì„ í™•ë¥ ë¡œ ë‚˜ëŠ” ë¹„ìœ¨ì…ë‹ˆë‹¤.

$$ \text{odds} = \frac{p \text{(ë°œìƒ ë¥ , í´ë˜ìŠ¤ 1)}}{1-p\text{(ë°œìƒX í™•ë¥ , í´ë˜ìŠ¤ 0)}}  $$

ì´ëŠ” Featureì˜ ê°¯ìˆ˜ê°€ ì¦ê°€í•  ìˆ˜ë¡ ë¡œì§“ì€ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ì§€ë¥¼ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì¦‰, Featureì˜ ê°¯ìˆ˜ê°€ ì¦ê°€í•  ìˆ˜ë¡ í™•ë¥ ì´ Pë°° ì¦ê°€í•œë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[ğŸ”— ë¡œì§€ìŠ¤í‹± íšŒê·€ - ê¸°ë‚´ì‹ì€ìˆ˜ë°•ë°”](https://soobarkbar.tistory.com/12?category=793437)

## ë¡œì§€ìŠ¤í‹± íšŒê·€
- [ë¡œì§€ìŠ¤í‹± íšŒê·€](https://en.wikipedia.org/wiki/Logistic_regression#Probability_of_passing_an_exam_versus_hours_of_study)ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(Sigmoid Function)ë¥¼ í†µí•´ **ì„ í˜•í•¨ìˆ˜ë¥¼ 0ê³¼ 1ì‚¬ì´ë¡œ ë°”ê¾¼ ê²ƒ**ìœ¼ë¡œ ê³µì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$ S(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1} $$

$$ H(x) = \frac{1}{1+e^{-(W_x + b)}} = S(W_x + b) = \sigma(W_x + b) $$

```py
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
frim sklearn.metrics import accuracy_score

model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_val_scaled)
accuracy_score(y_val, y_pred)

### íŠ¹ì„± ì¤‘ìš” ê³„ìˆ˜ í™•ì¸í•˜ê¸°
coeff = pd.Series(model.coef_[0], X_train_encoded.columns)
coeff

coeff.sort_values().plot.barh();
```

> #### ğŸ’¡ ë¶„ë¥˜ë¬¸ì œì— ì„ í˜•íšŒê·€ê°€ ì í•©í•˜ì§€ ì•ŠëŠ” ì´ìœ 
> - ì˜ˆì¸¡ ê°’ì´ í™•ë¥ ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ì˜ë¯¸ ìˆëŠ” ì„ê³„ê°’ì´ ì—†ìŠµë‹ˆë‹¤.
> - ë‹¤ì¤‘ í´ë˜ìŠ¤ë¥¼ ê°€ì§€ëŠ” ë¶„ë¥˜ ë¬¸ì œë¡œ í™•ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
> - xê°’ì— ë„ˆë¬´ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
> - [ğŸ”— ì„ í˜•íšŒê·€ê°€ ë¶„ë¥˜ ë¬¸ì œì— ì•ˆë˜ëŠ” ì´ìœ  - TooTouch](https://tootouch.github.io/IML/logistic_regression/#ì„ í˜•-íšŒê·€ê°€-ë¶„ë¥˜-ë¬¸ì œì—-ì•ˆë˜ëŠ”-ì´ìœ ëŠ”-ë¬´ì—‡ì¸ê°€)
> - [ğŸ”— ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ - cdjsì˜ ì½”ë”© ê³µë¶€ë°©](https://cding.tistory.com/55)


# Reference
- [ğŸ’» ë¡œì§€ìŠ¤í‹± íšŒê·€ ì‹¤ìŠµ ìë£Œ]()
- [ğŸ”— 5 Reasons â€œLogistic Regressionâ€ should be the first thing you learn when becoming a Data Scientist](https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4)
- [ğŸ“¼ Logistic Regression Details Pt1: Coefficients](https://youtu.be/vN5cNN2-HWE)
- [ğŸ“¼ Logistic Regression Details Pt 2: Maximum Likelihood](https://youtu.be/BfKanl1aSG0)
- [ğŸ”— ë¡œì§€ìŠ¤í‹± íšŒê·€ ê³µì‹ë¬¸ì„œ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
