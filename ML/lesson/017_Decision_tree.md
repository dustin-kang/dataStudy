# ê²°ì •íŠ¸ë¦¬ (Decision Trees)

> ê²°ì •íŠ¸ë¦¬ëŠ” ì´ì „ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)ê³¼ ë‹¤ë¥´ê²Œ **ë¹„ì„ í˜• ëª¨ë¸**ì´ë©° ë¹„ìš©í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ **ë…¸ë“œë¥¼ ë¶„í• í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜** ì…ë‹ˆë‹¤.

<img src="https://user-images.githubusercontent.com/55238671/214507276-693775b6-0556-43e5-af82-f6a55bacf9e1.jpg" width=500>


- ê²°ì •íŠ¸ë¦¬ êµ¬ì¡°ëŠ” íŠ¹ì • ìˆ˜ì¹˜ë¥¼ ê°€ì§€ê³  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ì•„ê°€ëŠ” ìŠ¤ë¬´ê³ ê°œì™€ ë¹„ìŠ·í•œ ë°©ì‹ì…ë‹ˆë‹¤. 
- ê²°ì •íŠ¸ë¦¬ëŠ” íšŒê·€, ë¶„ë¥˜ ë¬¸ì œì— ë‘˜ë‹¤ ì ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- [ğŸ”— ê²°ì •íŠ¸ë¦¬ í™•ì¸í•˜ê¸°](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

## ìˆœë„ì™€ ë¶ˆìˆœë„
ê²°ì •íŠ¸ë¦¬ë¥¼ ì´ìš©í•´ ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸°ìœ„í•´ **ì§€ë‹ˆë¶ˆìˆœë„(Gini Impurity)** ë¼ëŠ” ê°œë…ì„ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. _ì´ëŠ” ì •ë³´ì´ë“(IG)ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ ë§ì…ë‹ˆë‹¤._

<img src="https://user-images.githubusercontent.com/55238671/214506838-30cd10e2-c6e3-4032-ba8c-be8f258957a2.png" width=500>

> - ì •ë³´íšë“ = ì—”íŠ¸ë¡œí”¼ ê°ì†ŒëŸ‰
> - ë¶ˆìˆœë„ = ì—”íŠ¸ë¡œí”¼
> - ì •ë³´íšë“ = ë¶„í•  ì „ ë…¸ë“œ ë¶ˆìˆœë„ - ë¶„í•  í›„ ë…¸ë“œ ë¶ˆìˆœë„

- ë§Œì•½ ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ë°ì´í„°ë“¤ì´ í•œ ê³µê°„ì— ì ê²Œ ëª¨ì—¬ ìˆì„ ê²½ìš°, ìˆœë„ê°€ ë†’ë‹¤ê³  ì˜ë¯¸í•©ë‹ˆë‹¤. (ë§ì´ ëª¨ì—¬ìˆìœ¼ë©´ ë¶ˆìˆœë„ê°€ ë†’ê² ì£ .)
- ë¸”ìˆœë„ì˜ ê°ì†Œ ì •ë„ëŠ” ìƒëŒ€ì ì´ì§€ë§Œ, ë¶ˆìˆœë„ê°€ ê°€ì¥ ë§ì´ ê°ì†Œë ìˆ˜ë¡ ë§ì´ ìˆœìˆ˜í•´ì§€ê¸° ë–„ë¬¸ì— ê·¸ë§Œí¼ í™•ì‹¤íˆ ë¶„ë¥˜ëœ ì •ë³´ë¥¼ ë§ì´ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ë¶„í• ì— ì‚¬ìš©í•  Feature(Root Node)ì€ íƒ€ê²Ÿë³€ìˆ˜(Target)ì„ ì˜ êµ¬ë³„í•˜ëŠ”(ì •ë³´ íšë“ì´ í°) ê²ƒì„ ì„ íƒí•©ë‹ˆë‹¤.**


## ê²°ì •íŠ¸ë¦¬ 

> ğŸ“Œ ê²°ì •íŠ¸ë¦¬ ëª¨ë¸ì€ **í‘œì¤€í™”(StandardScaler)ë¥¼ í•  í•„ìš” ì—†ëŠ”** ê·œì§ ê¸°ë°˜(Rule-based) ëª¨ë¸ì…ë‹ˆë‹¤. ìœ ì˜í•˜ì„¸ìš”!

```py
from sklearn.tree import DecisionTreeClassifier # ê²°ì •íŠ¸ë¦¬ ê°ì²´ ì„í¬íŠ¸

model = DecisionTreeClassifier(random_state=2, criterion='entropy')
# criterion : ë…¸ë“œë¥¼ ë¶„í• í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ 'entropy'ë¥¼ ì§€ì •í•¨.

model.fit(X_train, y_train) # ê²°ì •íŠ¸ë¦¬ ëª¨ë¸ í•™ìŠµ
y_pred = model.predict(X_val) # ê²€ì¦ Features ë°ì´í„° ì…‹ìœ¼ë¡œ ì˜ˆì¸¡.
axxuracy_score(y_val, y_pred) # ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ

y_val.value_count(normalize=True) # ê¸°ì¤€ ëª¨ë¸(y_val)ì˜ ì„±ëŠ¥ì„ ë¹„êµ


# conda install -c conda-forge python-graphviz 

import graphviz
from sklearn.tree import export_graphviz

columns = X_val.columns # ê²€ì¦ Features ë°ì´í„° ì…‹ì˜ Featuresë¥¼ ë³€ìˆ˜ë¡œ ì§€ì •

dot_data = export_graphviz(model,
                        max_depth = 3,
                        feature_names = columns, # íŠ¸ë¦¬ ì‹œê°í™”
                        class_names = ['no','yes'],
                        filled = True,
                        proportion=True)

display(graphviz.Source(dot_data))

```
- `criterion` : ë…¸ë“œë¥¼ ë¶„í• í•˜ëŠ” ë°©ë²•
- `min_samples_split`Â : ë¶„ê¸°ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ìµœì†Œí•œ ì–¼ë§ˆë‚˜ ë˜ëŠ” ìƒ˜í”Œì´ ë…¸ë“œì— ìˆëŠ”ì§€ í™•ì¸
- `min_samples_leaf`Â : **ë§ˆì§€ë§‰ ë¦¬í”„ë…¸ë“œì— ìµœì†Œí•œ ëª‡ê°œì˜ ìƒ˜í”Œ**ì„ ì§€ì •í• ì§€
- `max_depth`Â : íŠ¸ë¦¬ì˜ **ì „ì²´ì ì¸ ê¹Šì´**ë¥¼ ì„¤ì •


> ğŸ“Œ ë‚˜ë­‡ê°€ì§€ê°€ ë§ì•„ì§€ë©´ [ê³¼ì í•©(Overfitting)](https://github.com/dustin-kang/dataStudy/blob/main/ML/lesson/007_ëª¨ë¸_í‰ê°€ì™€_ëª¨ë¸_ê°œì„ .md#ê³¼ëŒ€ì í•©overfitting)ì´ ì¼ì–´ë‚  í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤.

# Reference
- [ğŸ”— DecisionTree Classifier ê³µì‹ë¬¸ì„œ](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)
- [ğŸ”— DecisionTree Regressor ê³µì‹ë¬¸ì„œ](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)
- [ğŸ”— Random Forests for Complete Beginners The definitive guide to Random Forests and Decision ](https://victorzhou.com/blog/intro-to-random-forests/)
- [ğŸ”— ê²°ì •íŠ¸ë¦¬ì˜ ì¥ë‹¨ì ](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)
