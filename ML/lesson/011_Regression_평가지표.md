# íšŒê·€ (Regression)
íšŒê·€ ëª¨ë¸ì€ ë‘ê°œ ì´ìƒì˜ ë…ë¦½ë³€ìˆ˜(ì›ì¸)ì— ë”°ë¼ ì¢…ì†ë³€ìˆ˜(ê²°ê³¼)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
ëŒ€í‘œì ì¸ íšŒê·€ ëª¨ë¸ë¡œëŠ” ì„ í˜•íšŒê·€(Linear Regression), ê²°ì • íŠ¸ë¦¬(Decision Tree), kNN ë“±ì´ ì¡´ì¬ í•©ë‹ˆë‹¤.

> #### í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (Pearson Correlation)
> í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ëŠ” ì„ í˜• ê´€ê³„ì˜ ê°•ë„ì™€ ë°©í–¥ì„ ë‚˜íƒ€ë‚´ë©° -1ë¶€í„° 1ì‚¬ì´ì˜ ê°’ì„ í‘œì‹œí•©ë‹ˆë‹¤. (0ì— ê°€ê¹ë‹¤ë©´ ì„ í˜•ì  ê´€ê³„ê°€ ì—†ë‹¤ëŠ” ê²ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë¹„ì„ í˜•ì  ê´€ê³„ë¥¼ ê°€ì§ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.)
> 
> <img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FDxp4J%2FbtqCsIidh0q%2FaYvZ2Z8sdIaTd6IV4958b1%2Fimg.png" width=300>

## íšŒê·€ ëª¨ë¸
ìµœì ì˜ íšŒê·€ ëª¨ë¸ì€ ê°€ì¥ ì˜ë§ëŠ” ìµœì ì˜ ì„ ì„ ê¸‹ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ, **ìµœì ì˜ ì„ **ì€ **ì”ì°¨(residual) ì œê³±ì˜ í•©ì˜ ìµœì†Œí™”**í•˜ëŠ” ì§ì„ ì„ ì˜ë¯¸í•˜ëŠ”ë°ìš”. ì´ë¥¼ íšŒê·€ì„ ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. 

> - ì”ì°¨(residual) : ì˜ˆì¸¡ê°’ - ì‹¤ì œ ê´€ì¸¡ê°’ (ë¹„ìš© ë˜ëŠ” ì†ì‹¤ì´ë¼ ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.)
> - ì”ì°¨ ì œê³±ì˜ í•©(RSS: Residual sum of Squares = SSE)
> $$RSS = \sum_{i=1}^n (y_i - (\alpha x_i + \beta))^2$$
> - ì¦‰, ì”ì°¨ ì œê³±ì˜ í•©ì„ ìµœì†Œí™” í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— **íšŒê·€ ëª¨ë¸ì˜ ë¹„ìš©í•¨ìˆ˜**ê°€ ë˜ëŠ” ê±°ì£ .

<img width="284" alt="image" src="https://user-images.githubusercontent.com/55238671/212865134-deadf539-cf7b-44ee-9483-25bbfbc376a2.png">


## ìµœì†Œ ì œê³±ë²•(OLS, Ordinary Least Squares)
ìµœì†Œ ì œê³±ë²•ì´ ê°‘ìê¸° ë“±ì¥í–ˆì„ ê±°ë¼ ìƒê°í•˜ì§€ë§Œ, ì”ì°¨ ì œê³±ì˜ í•©ì„ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ì„ ìµœì†Œ ì œê³±í•© ë˜ëŠ” ìµœì†ŒììŠ¹ë²•ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ì¦‰, **ìµœì†Œ ì œê³±ë²•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì˜ í‘œí˜„í•˜ëŠ” ì„ í˜• íšŒê·€ì„ ì„ ê·¸ë¦´ ìˆ˜ ìˆëŠ”ê²ë‹ˆë‹¤.**

ì´ ê³µì‹ì„ í†µí•´ì„œ ìµœì†Œí™”í•˜ëŠ” ë³€ìˆ˜(ê°€ì¤‘ì¹˜)ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<img width="727" alt="image" src="https://user-images.githubusercontent.com/55238671/212865192-5a52f506-9b5f-4e4f-8fb1-adb197751968.png">


> ğŸ’¡ OLSëŠ” ì´ìƒì¹˜ì— ì˜í–¥ë ¥ì´ í¬ê¸° ë•Œë¬¸ì— ì´ìƒì¹˜ê°€ ìˆëŠ” ê²½ìš°ì—ëŠ” ê·¸ë‹¤ì§€ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì „ì²´ í•©ì— ì˜í–¥ì„ ì£¼ê¸° ë•Œë¬¸ì´ì£ .
>
> 


## í‰ê°€ ì§€í‘œ (Metrics)
### MAE(Mean Absolute Error, ì ˆëŒ€ í‰ê·  ì˜¤ì°¨)
  - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ë“¤ì˜ ì°¨ì´ì˜ **ì ˆëŒ€ê°’ì˜ í‰ê· **ì„ ë§í•©ë‹ˆë‹¤.
  - ë‹¨ìœ„ê°€ ê°™ì€ ë°ì´í„° í•´ì„ì— ìš©ì´í•©ë‹ˆë‹¤. (ë‹¨ìœ„ ìŠ¤ì¼€ì¼ì´ ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)
  - ì£¼ë¡œ, ëª¨ë¸ì„ êµ¬ë³„í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
  - `np.abs(np.substract(x,y)).mean()`


$$ \frac{1}{n}\sum_{i=1}^n|y_i - \hat y_i| $$

### MSE(Mean Square Error, í‰ê·  ì œê³± ì˜¤ì°¨)
  - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ë“¤ì˜ ì°¨ì´ì˜ **ì œê³±ì˜ í‰ê· **ì„ ë§í•©ë‹ˆë‹¤.
  - ì œê³±ì„ í–ˆê¸° ë•Œë¬¸ì— **ì´ìƒì¹˜ë‚˜ íŠ¹ì´ê°’ì— ë¯¼ê°**í•©ë‹ˆë‹¤.
  - ì–´ë–»ê²Œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
  - `np.square(np.substract(x,y)).mean()`


$$ \frac{1}{n}\sum_{i=1}^n(y_i - \hat y_i)^2 $$

### RMSE(Root Mean Square Error, í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨)
  - **MSEì˜ ì˜¤ë¥˜ ë¯¼ê°ë„ë¥¼ ê°œì„ **í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ë©°, ì œê³±ìœ¼ë¡œ ìŠ¤ì¼€ì¼ëœ ê°’ì„ ë˜ëŒë¦¬ëŠ”ë° ì‚¬ìš©í•©ë‹ˆë‹¤.
  - í° ì˜¤ë¥˜ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.


$$ \sqrt {MSE} $$

### RMSLE(Root Mean Square Logarithmic Error, í‰ê·  ì œê³±ê·¼ ë¡œê·¸ ì˜¤ì°¨)
  - RMSEì™€ ë¹„ìŠ·í•œ ê³µì‹ì´ì§€ë§Œ, ì˜ˆì¸¡ê°’ê³¼ ì •ë‹¶ê°’ì— **ê°ê° ë¡œê·¸ë¥¼ ì”Œì›Œ ê³„ì‚°**í•œë‹¤ëŠ” ê²ƒì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.

$$ \sqrt {\frac{1}{n}\sum_{i=1}^n(\log(y_i + 1) - \log(\hat y_i + 1))^2} $$


### R-Squared (coefficient of Determination, ê²°ì •ê³„ìˆ˜)
  - **ëª¨í˜•ì˜ ì„¤ëª…ë ¥**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
  - ë¶„ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œë¥¼ ë§í•˜ë©°, **1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì„¤ëª…ë ¥ì´ ì¢‹ë‹¤ê³  íŒë‹¨**í•œë‹¤.
  - íšŒê·€ì„ ì—ì„œ ì„¤ëª…ë˜ëŠ” ì˜¤ì°¨ / ì „ì²´ ì˜¤ì°¨

$$ 1 - \frac{\sum_{i=1}^n(y_i -\hat y_i)^2}{\sum_{i=1}^n(y_i -\bar y_i)^2} = 1 - \frac{SSE}{SST} = \frac{SSR}{SST} = \frac{\sum_{i=1}^n(\hat y_i -\bar y_i)}{\sum_{i=1}^n(y_i -\bar y_i)^2} $$


> - SSE(RSS) : ê´€ì¸¡ì¹˜ì™€ ì˜ˆì¸¡ì¹˜ ì°¨ì´ ì œê³± í‰ê· 
> - SSR : ì˜ˆì¸¡ì¹˜ì™€ í‰ê·  ì°¨ì´ ì œê³± í‰ê· 
> - SST : ê´€ì¸¡ì¹˜ì™€ í‰ê·  ì°¨ì´ SSE + SSR

### ìˆ˜ì •ëœ ê²°ì •ê³„ìˆ˜
ìœ„ ê³µì‹ì€ ì‹¤ì œë¡œ ëª¨í˜•ì´ ì„¤ëª…ë ¥ì´ ë†’ì€ ê±´ì§€ ì•„ë‹ˆë©´ ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ê°œìˆ˜ê°€ ë§ì€ ê±´ì§€ **ì‹ ë¢°ë¥¼ í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œ**ê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¥¼ **í•´ê²°**í•˜ê¸° ìœ„í•´ ì•„ë˜ ìˆ˜ì •ëœ ê²°ì •ê³„ìˆ˜ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

- í‘œë³¸ì˜ í¬ê¸°ëŠ” $n$, ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜ëŠ” $p$ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
- ì£¼ë¡œ ë‹¨ìˆœ íšŒê·€ë³´ë‹¤ ë‹¤ì¤‘ íšŒê·€ ë¶„ì„ì—ì„œ ì´ ì‹ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. 

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbzVAKA%2FbtqAZPjxN7O%2FRfR1KgULS95W2ETHkueYX1%2Fimg.png">

$$ AdjR^2 = 1 - \frac{n -1}{(n-p-1)(1-R^2)} $$

> ê²°ì •ê³„ìˆ˜ëŠ” ë…ë¦½ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê²°ì •ê³„ìˆ˜ê°€ 1ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.

## Reference
- [ğŸ”— DATA - 17. ìµœì†ŒììŠ¹ë²•(OLS)ì„ í™œìš©í•œ ë‹¨ìˆœ ì„ í˜• íšŒê·€ (Simple Linear Regression) - ê·€í‰ì´ ì„œì¬](https://bkshin.tistory.com/entry/DATA-17-Regression)
- [ğŸ”— [ê²°ì •ê³„ìˆ˜] R squareì™€ adjusted R square - specialsence](https://specialscene.tistory.com/63)
- [ğŸ”— 3 Best metrics to evaluate Regression Model? - Songhao Wu](https://towardsdatascience.com/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b)
- [ğŸ“¼ How to calculate linear regression using least square method](https://www.youtube.com/watch?v=JvS2triCgOY)
- [ğŸ“¼ An Introduction to Linear Regression Analysis](https://www.youtube.com/watch?v=zPG4NjIkCjc)
- [ğŸ”— Python Data Science Handbook, Chapter 5.2: Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Basics-of-the-API)
- [ğŸ”— 2.4.2.2. Supervised Learning](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/text_analytics/general_concepts.html#supervised-learning-model-fit-x-y)
- [ğŸ”— sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [ğŸ”— sklearn.metrics.mean_absolute_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)
