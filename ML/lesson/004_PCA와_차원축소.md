# 차원 축소 (Dimension Reduction)
### 차원의 저주 (Curse of Dimensionality)
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FboSJYB%2FbtraVTqkwC1%2FSyMcbBfsrOozbaoeKQ4ilK%2Fimg.png" width=500>

- **데이터 차원이 증가**할수록 해당 공간의 크기가 증가하여 데이터 간 거리가 멀어지면서 희소한 구조를 갖는 현상
- 시각화나 탐색 자체가 불가능해진다.
- 이를 해결하기 위해서는 차원을 증가시킨만큼 더 많은 데이터를 추가하거나 PCA, LDA, LLE, MDS와 같은 차원 축소 알고리즘으로 차원을 줄인다.

[🔗 차원의 저주 - BioinformaticsAndMe ](https://bioinformaticsandme.tistory.com/197)

## 차원 축소 기법의 종류
### Feature Selection
> 종속성이 강한 불필요한 피처를 제거하고 데이터를 잘표현하는 주요 피처들만 선택한다.
- 장점 : 해석의 용이성
- 단점 : 피처들간의 연관성을 고려해야한다. 
- 예시 : LASSO, Genetic Algorithm


### Feature Extraction
> 기존의 피처들을 저차원 형태로 압축하여 잘 설명할 수 있도록 매핑하는 것이다.
- 장점 : Feature 수를 줄일 수 있다.
- 단점 : 해석의 어려움
- 에시 : PCA, Auto-Encoder, One-Hot Encoding

> <details> <summary> 🔍 데이터 분산의 중요성 </summary> 
> 데이터 분산은 즉, 데이터의 정보입니다.
> 데이터가 다양할수록(분산이 높을수록) 정보량 또한 많아집니다.
> **분산이 높은 피처는 차원 축소하기 좋은 피처입니다.**
> </detail>

# PCA (주성분 분석)

## PC(Principal Component)
- 데이터의 변동을 최대한 설명해주고 공분산 구조에 대한 해석을 용이하도록 만들어주는 고유값


## PCA

<img src="https://user-images.githubusercontent.com/55238671/209341153-559603b1-1dd2-4cd5-8cac-a693108bbd51.png" width=600>


- 입력 데이터(Feature)의 **공분산 행렬을 기반으로 고유벡터를 생성하고 고유벡터에 입력 데이터를 선형 변환하여 차원을 축소하는 방법**입니다.
- 변수들간의 특징을 개별적으로 설명할 수 있는지를 알고자 할때 사용합니다.

> 🔍 PCA는 고유값이 큰(분산이 높은) 순으로 주성분 벡터를 추출하는데 가장 먼저 뽑힌 벡터는 가장 나중에 뽑힌 벡터보다 설명력이 높기 때문에 **노이즈 제거 방법**이라고도 불립니다.

[🔗 차원축소, PCA, LDA, LSA, SVD 간단 정리 - Hui_dea](https://huidea.tistory.com/126)

## 💻 PCA 라이브러리를 이용하여 계산하기
```py
from sklearn.decomposition import PCA

pca = PCA(n_components = 2) # 2차원 데이터로 차원 축소 객체를 생성

PCs = pca.fit(x) # 주성분 분석

print("\n Eigenvectors: \n", PCs.components_)
print("\n Eigenvalues: \n",PCs.explained_variance_)

PCs = pca.transform(x) # Projected Data
# or "pca.fit_transform(x)"

PCdata = pd.DataFrame(
    data = PCs,
    columns = ['PC1','PC2'] # 차원 축소한 컬럼
) # PCdata 데이터 프레임 화
```
- `pca = PCA(n)` : 차원축소 할 차원 수 설정
- `pcs = pca.fit(x)` : pca를 이용해 차원 주성분을 만들어냅니다.
- `pcs = pca.transform(x)` : 고유벡터에 프로젝션을 한 결과
- `pca.singular_values_` : 주성분의 고유값
- `pca.components_.T` : 고유벡터
- `pca.explained_variance_` : 분산
- `pca.explained_variance_ratio_` : 분산비율, 내용을 설명하기 위해 얼마만큼 정보량이 필요한지 알 수 있습니다.

[🔗 실습 파일 바로가기](https://github.com/dustin-kang/dataStudy/blob/main/ML/practice/004_PCA와_차원축소.ipynb)
[🔗 실습+ 파일 바로가기](https://github.com/dustin-kang/dataStudy/blob/main/ML/practice/004_PCA와_차원축소.ipynb)

> 🔍 Kernal PCA
> - PCA와 다르게 전자의 단점을 보완하여 선형 관계가 아닐때도 사용할 수 있다. (공분산이 아닌 관측치 사이의 구조를 파악한다.)

### Scree Plot

<img src="https://velog.velcdn.com/images%2Fyuns_u%2Fpost%2F95f51fc1-d4e4-4659-a0b4-10d585ef86ed%2Fimage.png" width=600>

- **주성분의 수를 선정하기 위해 고유값-주성분의 변화를 보는 그래프**이며, 고유값이 급하강 되거나 기여율이 높은 지점(7-80%)은 그만큼 정보를 유지할 수 있고 차원 축소도 진행할 수 있다.

[🔗 screeplot - yuns_u](https://velog.io/@yuns_u/PCA-Scree-Plot)
