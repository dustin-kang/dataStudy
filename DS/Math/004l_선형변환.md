# Linear Transformation
> ### 👨‍💻 개념 정리
> - **Domain(정의역)** : x, 함수의 입력값의 집합
> - **Co-Domain(공역)** : y, 함수의 출력값을 가질 수 있는 모든 집합
> - **Image(함수의 상)** : 입력이 주어졌을 때 출력값
> - **치약(Range)** : 실제로 y값으로 쓰이는 값의 집합

> 📌 함수의 정의역과 치역은 일대일 대응 관계를 가지므로 **하나의 화살표**를 통해 가리킵니다.

<img src="https://user-images.githubusercontent.com/55238671/211512232-9929c960-12be-4183-8472-fb96b14ebf98.jpg" width=400>

## 선형 변환(Linear Transformation)
> - 정의역에서 원소 두개를 뽑아 선형 결합이 나오는 함수(벡터) == 선형결합을 나중에 했을 때의 함수 값
> - 위 공식을 만족하면 선형 결합입니다.

<img src="https://user-images.githubusercontent.com/55238671/211512566-cea83f7f-fdeb-4990-afbd-9f3ae63292be.jpg" width=400>

- n차원 공간의 벡터를 m차원이라는 다른 차원의 **공간으로 변형하는 것**을 말합니다.


- 어떤 함수 또는 매핑이 선형이라면 아래 수식이 보장되어야 합니다.

$$ T = R^n \to R^m $$

$$ T(a_1x_1 + a_2x_2) = a_1T(x_1) + a_2T(x_2) $$

>  **y = 3x + 2 는 선형결합일까?**
> - x = 1, T(1) = 5
> - x = 2, T(2) = 8
> - T(3X1 + 4X2) = T(3XT(1) + 4XT(2))?
> - 35 != 47 (선형성을 보장하지 못함.)
> - 하지만 **정의역을 다르게 가지게 하여** 2차원에서 1차원으로 변환하면 보장합니다. 

### 벡터 사이의 변환
- 만약, 기저 벡터(정의역,x)들의 입력으로 선형 변환 결과(공역, y)를 알고 있다면 행렬 A를 알아낼 수 있습니다.
- 다른 기저 벡터로 변환된 결과라면 당연히 알아내지 못합니다.

$$ x \in \mathbb{R}^3 \to y \in \mathbb{R}^2 $$

## 신경망 속 선형 변환
> 신경망에서는 FC Layer에서 가중치와 입력값의 내적과 **bias(편향)에 의한 선형 변환**을 기하학적으로 볼 수 있습니다.

<img src= "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/1layer.gif" width=300>

위 그림을 보게되면 모눈종이가 **평행사변형의 모양으로 변경**되는 것을 알 수 있습니다. 이는 **선형변환에서 발생하는 경우**인데요.

- 0에서 멀어지는 부분은 압축을 시켜 값을 작게 만드는 역할입니다.
- 옆으로 뭉게지는 느낌은 bias-term 때문에 발생하는 경우입니다.

### Affine Layer(FC Layer) in Neural Networks
- FC Layer는 보통 bias term을 포함합니다. 이유는 선형을 만족하지 않은 Affine 공간으로 확장이 이루어지기 때문이죠.
- 이 때문에 선형 변환 형태로 계산하기 위해 **차원을 하나 더 늘려야 합니다.**







- [🔗 colah.github.io](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
- [🔗 https://heung-bae-lee.github.io](https://heung-bae-lee.github.io/2020/06/09/linear_algebra_05/)
- [🔗 WE DONE IT](https://wegonnamakeit.tistory.com/31)
